import os
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer

# --- Cáº¤U HÃŒNH ---
RAW_PATH = "..data"
OUTPUT_PATH = "data/processed"

# NgÆ°á»¡ng lá»c dá»¯ liá»‡u (Äá»ƒ giáº£m nhiá»…u vÃ  tÄƒng tá»‘c Ä‘á»™ train)
MIN_USER_RATINGS = 5   # User pháº£i rate Ã­t nháº¥t 5 phim má»›i Ä‘Æ°á»£c giá»¯ láº¡i
MIN_MOVIE_RATINGS = 10 # Phim pháº£i cÃ³ Ã­t nháº¥t 10 ngÆ°á»i rate má»›i Ä‘Æ°á»£c giá»¯ láº¡i
MIN_RATING_FOR_RULES = 3.5 # Rating >= 3.5 Ä‘Æ°á»£c coi lÃ  "ThÃ­ch" cho luáº­t káº¿t há»£p

def main():
    # 1. Khá»Ÿi táº¡o Spark vá»›i cáº¥u hÃ¬nh bá»™ nhá»› cao (cho data 32M)
    print("ğŸš€ Äang khá»Ÿi Ä‘á»™ng Spark Session...")
    spark = SparkSession.builder \
        .appName("MovieLens_Preprocessing") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .getOrCreate()

    # 2. Load dá»¯ liá»‡u thÃ´
    print("ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»« CSV...")
    df_ratings = spark.read.csv(os.path.join(RAW_PATH, "ratings.csv"), header=True, inferSchema=True)
    df_movies = spark.read.csv(os.path.join(RAW_PATH, "movies.csv"), header=True, inferSchema=True)
    df_links = spark.read.csv(os.path.join(RAW_PATH, "links.csv"),header=True,inferSchema=True)


    # 3. Lá»c Dá»¯ liá»‡u chung (Global Filtering)
    # Loáº¡i bá» dá»¯ liá»‡u "rÃ¡c" (Sparse data) giÃºp cáº£ 3 model cháº¡y nhanh vÃ  chÃ­nh xÃ¡c hÆ¡n
    print(f"ğŸ§¹ Äang lá»c dá»¯ liá»‡u (Min User Rate: {MIN_USER_RATINGS}, Min Movie Rate: {MIN_MOVIE_RATINGS})...")
    
    # Äáº¿m sá»‘ rating cá»§a user vÃ  movie
    user_counts = df_ratings.groupBy("userId").count().withColumnRenamed("count", "user_count")
    movie_counts = df_ratings.groupBy("movieId").count().withColumnRenamed("count", "movie_count")
    
    # Lá»c
    df_clean = df_ratings \
        .join(user_counts, "userId", "inner").filter(F.col("user_count") >= MIN_USER_RATINGS) \
        .join(movie_counts, "movieId", "inner").filter(F.col("movie_count") >= MIN_MOVIE_RATINGS) \
        .select("userId", "movieId", "rating", "timestamp") # Chá»‰ giá»¯ láº¡i cá»™t cáº§n thiáº¿t

    # Cache láº¡i vÃ o RAM vÃ¬ biáº¿n nÃ y sáº½ dÃ¹ng cho cáº£ 3 nhÃ¡nh
    df_clean.cache()
    print(f"âœ… Dá»¯ liá»‡u sáº¡ch cÃ²n láº¡i: {df_clean.count()} dÃ²ng rating.")

    # ==========================================
    # NHÃNH 1: Xá»¬ LÃ CHO MODEL ASSOCIATION RULES
    # ==========================================
    print("\nğŸ›   Äang xá»­ lÃ½ dá»¯ liá»‡u cho Model 1 (Association Rules)...")
    # Logic: Chá»‰ láº¥y phim User thÃ­ch -> Gom thÃ nh list tÃªn phim
    
    # Láº¥y rating cao vÃ  join vá»›i tÃªn phim
    df_rules = df_clean.filter(F.col("rating") >= MIN_RATING_FOR_RULES) \
        .join(df_movies, "movieId", "inner")
    
    # Gom nhÃ³m: User | [Phim A, Phim B, Phim C]
    df_transactions = df_rules.groupBy("userId") \
        .agg(F.collect_list("title").alias("items"))
    
    # LÆ°u
    path_m1 = os.path.join(OUTPUT_PATH, "model1_rules")
    df_transactions.write.mode("overwrite").parquet(path_m1)
    print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u Model 1 táº¡i: {path_m1}")

    # ==========================================
    # NHÃNH 2: Xá»¬ LÃ CHO MODEL SPARK ALS
    # ==========================================
    print("\nğŸ›   Äang xá»­ lÃ½ dá»¯ liá»‡u cho Model 2 (Spark ALS)...")
    # Logic: Giá»¯ nguyÃªn dáº¡ng sá»‘ (userId, movieId, rating). Spark ALS tá»± xá»­ lÃ½ Ä‘Æ°á»£c ID rá»i ráº¡c.
    
    path_m2 = os.path.join(OUTPUT_PATH, "model2_als")
    df_clean.select("userId", "movieId", "rating").write.mode("overwrite").parquet(path_m2)
    print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u Model 2 táº¡i: {path_m2}")

    # ==========================================
    # NHÃNH 3: Xá»¬ LÃ CHO MODEL NEURAL CF
    # ==========================================
    print("\nğŸ›   Äang xá»­ lÃ½ dá»¯ liá»‡u cho Model 3 (Deep Learning)...")
    # Logic: Deep Learning cáº§n index liÃªn tá»¥c tá»« 0 -> N. 
    # userId gá»‘c: 1, 50, 100 -> userId má»›i: 0, 1, 2
    
    # Indexing User
    user_indexer = StringIndexer(inputCol="userId", outputCol="userIndex")
    user_indexer_model = user_indexer.fit(df_clean)
    df_indexed = user_indexer_model.transform(df_clean)
    
    # Indexing Movie
    movie_indexer = StringIndexer(inputCol="movieId", outputCol="movieIndex")
    movie_indexer_model = movie_indexer.fit(df_indexed)
    df_final_ncf = movie_indexer_model.transform(df_indexed)
    
    # LÆ°u data Ä‘Ã£ index (Cast vá» integer cho nháº¹)
    df_final_ncf = df_final_ncf.select(
        F.col("userIndex").cast("integer"), 
        F.col("movieIndex").cast("integer"), 
        F.col("rating")
    )
    
    path_m3 = os.path.join(OUTPUT_PATH, "model3_ncf")
    df_final_ncf.write.mode("overwrite").parquet(path_m3)
    
    # QUAN TRá»ŒNG: LÆ°u láº¡i map Ä‘á»ƒ sau nÃ y tra ngÆ°á»£c (Index 0 lÃ  phim gÃ¬?)
    # LÆ°u danh sÃ¡ch user gá»‘c (index tÆ°Æ¡ng á»©ng lÃ  vá»‹ trÃ­ trong máº£ng)
    print("   -> Äang lÆ°u mapping User/Item...")
    spark.createDataFrame([(i, ) for i in user_indexer_model.labels], ["original_userId"]) \
        .write.mode("overwrite").parquet(os.path.join(OUTPUT_PATH, "model3_ncf_user_mapping"))
        
    spark.createDataFrame([(i, ) for i in movie_indexer_model.labels], ["original_movieId"]) \
        .write.mode("overwrite").parquet(os.path.join(OUTPUT_PATH, "model3_ncf_movie_mapping"))

    print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u Model 3 táº¡i: {path_m3}")

    spark.stop()
    print("\nğŸ‰ Xá»¬ LÃ HOÃ€N Táº¤T! Sáºµn sÃ ng Ä‘á»ƒ train.")

if __name__ == "__main__":
    main()