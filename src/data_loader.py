import os
import shutil
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import ArrayType, StringType

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
# ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n tr·ªè ƒë√∫ng t·ªõi folder ch·ª©a file g·ªëc c·ªßa b·∫°n
RAW_MOVIES_PATH = "data/raw/movies.csv"
RAW_TAGS_PATH = "data/raw/tags.csv"
OUTPUT_PATH = "checkpoints/metadata.parquet"

def main():
    print("üöÄ Kh·ªüi ƒë·ªông Spark ƒë·ªÉ t·∫°o Metadata...")
    spark = SparkSession.builder \
        .appName("Create_Metadata_Parquet") \
        .config("spark.driver.memory", "4g") \
        .master("local[*]") \
        .getOrCreate()

    # 1. ƒê·ªåC D·ªÆ LI·ªÜU
    print("üìÇ ƒêang ƒë·ªçc movies.csv v√† tags.csv...")
    if not os.path.exists(RAW_MOVIES_PATH):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y {RAW_MOVIES_PATH}")
        return

    df_movies = spark.read.csv(RAW_MOVIES_PATH, header=True, inferSchema=True)
    
    # ƒê·ªçc tags (N·∫øu kh√¥ng c√≥ file tags th√¨ t·∫°o DataFrame r·ªóng ƒë·ªÉ kh√¥ng b·ªã l·ªói code)
    if os.path.exists(RAW_TAGS_PATH):
        df_tags = spark.read.csv(RAW_TAGS_PATH, header=True, inferSchema=True)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y tags.csv, s·∫Ω ch·ªâ d√πng Genres.")
        df_tags = spark.createDataFrame([], schema="userId INT, movieId INT, tag STRING, timestamp LONG")

    # 2. L√ÄM S·∫†CH T√äN PHIM (QUAN TR·ªåNG: PH·∫¢I KH·ªöP V·ªöI PROCESS_DATA.PY)
    # Logic: Chuy·ªÉn "Matrix, The (1999)" -> "The Matrix (1999)"
    # D√πng h√†m Spark Native cho nhanh
    print("üßπ ƒêang chu·∫©n h√≥a t√™n phim...")
    df_movies = df_movies.withColumn(
        "title",
        F.when(
            F.col("title").rlike(r'^(.*),\s(The|A|An|Les|Le|La)\s\((\d{4})\)$'),
            F.regexp_replace(F.col("title"), r'^(.*),\s(The|A|An|Les|Le|La)\s\((\d{4})\)$', '$2 $1 ($3)')
        ).otherwise(
            F.when(
                F.col("title").rlike(r'^(.*),\s(The|A|An|Les|Le|La)$'),
                F.regexp_replace(F.col("title"), r'^(.*),\s(The|A|An|Les|Le|La)$', '$2 $1')
            ).otherwise(F.col("title"))
        )
    )

    # 3. X·ª¨ L√ù GENRES (TH·ªÇ LO·∫†I)
    # Input: "Adventure|Animation|Children|Comedy|Fantasy"
    # Output: ["Adventure", "Animation", "Children", "Comedy", "Fantasy"]
    print("üé® ƒêang x·ª≠ l√Ω Genres...")
    df_movies = df_movies.withColumn(
        "genres_arr", 
        F.split(F.col("genres"), "\|") # T√°ch chu·ªói b·∫±ng d·∫•u g·∫°ch ƒë·ª©ng
    )

    # 4. X·ª¨ L√ù TAGS (TH·∫∫ T·ª™ NG∆Ø·ªúI D√ôNG)
    print("üè∑Ô∏è  ƒêang x·ª≠ l√Ω Tags...")
    # - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    # - Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    # - Gom nh√≥m theo MovieId -> T·∫°o list c√°c tag kh√¥ng tr√πng l·∫∑p (collect_set)
    df_tags_clean = df_tags.filter(F.col("tag").isNotNull()) \
        .withColumn("tag_clean", F.lower(F.trim(F.col("tag")))) \
        .groupBy("movieId") \
        .agg(F.collect_set("tag_clean").alias("tags_arr"))

    # 5. G·ªòP D·ªÆ LI·ªÜU (JOIN)
    print("üîó ƒêang g·ªôp Movies v√† Tags...")
    # Join Left: Gi·ªØ l·∫°i t·∫•t c·∫£ phim k·ªÉ c·∫£ phim kh√¥ng c√≥ tag
    df_final = df_movies.join(df_tags_clean, on="movieId", how="left")

    # 6. T·∫†O C·ªòT FEATURES (GENRES + TAGS)
    # N·∫øu tags_arr l√† null (do kh√¥ng c√≥ tag), thay b·∫±ng m·∫£ng r·ªóng ƒë·ªÉ kh√¥ng l·ªói khi c·ªông
    df_final = df_final.withColumn(
        "tags_arr", 
        F.coalesce(F.col("tags_arr"), F.array().cast(ArrayType(StringType())))
    )
    
    # G·ªôp 2 m·∫£ng l·∫°i: features = genres_arr + tags_arr
    df_final = df_final.withColumn(
        "features", 
        F.array_distinct(F.concat(F.col("genres_arr"), F.col("tags_arr")))
    )

    # Ch·ªâ l·∫•y c·ªôt c·∫ßn thi·∫øt
    df_output = df_final.select("movieId", "title", "features")

    # 7. L∆ØU FILE PARQUET
    print(f"üíæ ƒêang l∆∞u Metadata t·∫°i: {OUTPUT_PATH}")
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c cha t·ªìn t·∫°i
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    
    # X√≥a folder c≈© n·∫øu c√≥ ƒë·ªÉ tr√°nh l·ªói
    if os.path.exists(OUTPUT_PATH):
        # shutil.rmtree(OUTPUT_PATH) # C·∫©n th·∫≠n v·ªõi l·ªánh n√†y tr√™n server th·∫≠t
        pass 

    df_output.write.mode("overwrite").parquet(OUTPUT_PATH)
    
    print("‚úÖ ƒê√É T·∫†O METADATA TH√ÄNH C√îNG!")
    print("üìä M·∫´u d·ªØ li·ªáu:")
    df_output.show(5, truncate=False)
    
    spark.stop()

if __name__ == "__main__":
    main()