import os
import re
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer
from pyspark.sql.types import StringType
from pyspark.sql.window import Window

# --- C·∫§U H√åNH ---
RAW_PATH = "data/raw"
OUTPUT_PATH = "data/processed"

# C√°c th∆∞ m·ª•c con t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng Model
MODEL_PATHS = {
    "m1": os.path.join(OUTPUT_PATH, "model1_rules"),
    "m2": os.path.join(OUTPUT_PATH, "model2_als"),
}

# Ng∆∞·ª°ng l·ªçc d·ªØ li·ªáu
MIN_USER_RATINGS = 5
MIN_MOVIE_RATINGS = 10
MIN_RATING_FOR_RULES = 3.5

def clean_title_logic(title):
    if not title: return title
    title = title.strip()
    match = re.match(r'^(.*),\s(The|A|An|Les|Le|La)\s\((\d{4})\)$', title)
    if match: return f"{match.group(2)} {match.group(1)} ({match.group(3)})"
    match_no_year = re.match(r'^(.*),\s(The|A|An|Les|Le|La)$', title)
    if match_no_year: return f"{match_no_year.group(2)} {match_no_year.group(1)}"
    return title

def check_processed_data():
    """Ki·ªÉm tra xem d·ªØ li·ªáu s·∫°ch cho c·∫£ 3 model ƒë√£ t·ªìn t·∫°i ch∆∞a"""
    if not os.path.exists(OUTPUT_PATH):
        return False
    
    for name, path in MODEL_PATHS.items():
        # Ki·ªÉm tra th∆∞ m·ª•c c√≥ t·ªìn t·∫°i v√† c√≥ ch·ª©a file (kh√¥ng r·ªóng) kh√¥ng
        if not os.path.exists(path) or not os.listdir(path):
            print(f"‚ö†Ô∏è  Thi·∫øu d·ªØ li·ªáu t·∫°i: {path}")
            return False
    return True

def main():
    # 1. KI·ªÇM TRA D·ªÆ LI·ªÜU S·∫†CH TR∆Ø·ªöC
    data_exists = check_processed_data()
    
    if data_exists:
        print("‚úÖ D·ªØ li·ªáu s·∫°ch (Processed Data) ƒë√£ t·ªìn t·∫°i ƒë·∫ßy ƒë·ªß.")
        return
    else:
        print("Status: üÜï D·ªØ li·ªáu s·∫°ch ch∆∞a c√≥ ho·∫∑c ch∆∞a ƒë·ªß. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")

    # 2. KI·ªÇM TRA D·ªÆ LI·ªÜU TH√î (RAW) TR∆Ø·ªöC KHI B·∫¨T SPARK
    required_raw = ["ratings.csv", "movies.csv"]
    for f in required_raw:
        if not os.path.exists(os.path.join(RAW_PATH, f)):
            print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y {f} trong {RAW_PATH}. Vui l√≤ng ki·ªÉm tra l·∫°i!")
            sys.exit(1)

    # 3. KH·ªûI T·∫†O SPARK SESSION
    print("\nüöÄ ƒêang kh·ªüi ƒë·ªông Spark Session (C·∫•p ph√°t 8GB RAM)...")
    spark = SparkSession.builder \
        .appName("MovieLens_Preprocessing") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .config("spark.sql.shuffle.partitions", "200") \
        .getOrCreate()

    try:
        # --- ƒê·ªåC D·ªÆ LI·ªÜU ---
        print("üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu th√¥...")
        df_ratings = spark.read.csv(os.path.join(RAW_PATH, "ratings.csv"), header=True, inferSchema=True)
        df_movies = spark.read.csv(os.path.join(RAW_PATH, "movies.csv"), header=True, inferSchema=True)
        clean_title_udf = F.udf(clean_title_logic, StringType())
        df_movies = df_movies.withColumn("title", clean_title_udf(F.col("title")))

        # --- L·ªåC D·ªÆ LI·ªÜU CHUNG ---
        print(f"üßπ ƒêang l·ªçc (User >= {MIN_USER_RATINGS} rates, Movie >= {MIN_MOVIE_RATINGS} rates)...")
        user_counts = df_ratings.groupBy("userId").count().withColumnRenamed("count", "user_count")
        movie_counts = df_ratings.groupBy("movieId").count().withColumnRenamed("count", "movie_count")
        
        df_clean = df_ratings \
            .join(user_counts, "userId", "inner").filter(F.col("user_count") >= MIN_USER_RATINGS) \
            .join(movie_counts, "movieId", "inner").filter(F.col("movie_count") >= MIN_MOVIE_RATINGS) \
            .select("userId", "movieId", "rating", "timestamp")

        df_clean.cache()
        print(f"‚úÖ D·ªØ li·ªáu sau l·ªçc: {df_clean.count()} d√≤ng.")

        # --- NH√ÅNH 1: ASSOCIATION RULES ---
        print("\nüõ†  Model 1 (Rules) - Hybrid Selection (Best + Recent)...")
    
        # B∆∞·ªõc 1: Ch·ªâ l·∫•y phim User ƒë√£ th√≠ch (Rating >= 3.5)
        # Lo·∫°i b·ªè phim d·ªü ƒë·ªÉ tr√°nh h·ªçc lu·∫≠t sai
        df_high_rating = df_clean.filter(F.col("rating") >= 3.5)
        
        # Join l·∫•y t√™n phim
        df_rules_joined = df_high_rating.join(df_movies, "movieId", "inner") \
                                        .select("userId", "title", "rating", "timestamp")
        
        # C·ª≠a s·ªï 1: X·∫øp theo Rating gi·∫£m d·∫ßn (∆Øu ti√™n phim 5 sao)
        # N·∫øu rating b·∫±ng nhau, phim n√†o m·ªõi h∆°n x·∫øp tr∆∞·ªõc
        w_best = Window.partitionBy("userId").orderBy(F.col("rating").desc(), F.col("timestamp").desc())
        
        # C·ª≠a s·ªï 2: X·∫øp theo Th·ªùi gian gi·∫£m d·∫ßn (∆Øu ti√™n phim m·ªõi xem)
        w_recent = Window.partitionBy("userId").orderBy(F.col("timestamp").desc())
        
        df_ranked = df_rules_joined \
            .withColumn("rank_best", F.row_number().over(w_best)) \
            .withColumn("rank_recent", F.row_number().over(w_recent))
        
        # L·∫§Y H·ª¢P (UNION) C·ª¶A 2 NH√ìM:
        # - Nh√≥m 1: Top 30 phim hay nh·∫•t (Gi·ªØ huy·ªÅn tho·∫°i)
        # - Nh√≥m 2: Top 20 phim m·ªõi nh·∫•t (Gi·ªØ xu h∆∞·ªõng)
        # T·ªïng c·ªông t·ªëi ƒëa 50 phim/user (N·∫øu phim v·ª´a hay v·ª´a m·ªõi th√¨ c√†ng t·ªët)
        df_smart_selected = df_ranked.filter(
            (F.col("rank_best") <= 50) 
        )
        
        # B∆∞·ªõc 3: Gom nh√≥m
        # D√πng collect_set ƒë·ªÉ t·ª± ƒë·ªông kh·ª≠ tr√πng l·∫∑p (n·∫øu phim n·∫±m trong c·∫£ 2 top)
        df_transactions = df_smart_selected.groupBy("userId") \
                                        .agg(F.collect_set("title").alias("items"))
        
        # L∆∞u ra disk
        path_m1 = os.path.join(OUTPUT_PATH, "model1_rules")
        df_transactions.write.mode("overwrite").parquet(path_m1)
        
        print(f"‚úÖ Model 1 Saved: ƒê√£ ch·ªçn l·ªçc tinh hoa .")
        # --- NH√ÅNH 2: SPARK ALS ---
        print("üõ†  X·ª≠ l√Ω Model 2 (Spark ALS)...")
        df_clean.select("userId", "movieId", "rating").write.mode("overwrite").parquet(MODEL_PATHS["m2"])

        # --- NH√ÅNH 3: NEURAL CF (DEEP LEARNING) ---
        # print("üõ†  X·ª≠ l√Ω Model 3 (Neural CF)...")
        # # Indexing li√™n t·ª•c cho User v√† Movie
        # u_indexer = StringIndexer(inputCol="userId", outputCol="userIndex").fit(df_clean)
        # m_indexer = StringIndexer(inputCol="movieId", outputCol="movieIndex").fit(df_clean)
        
        # df_ncf = u_indexer.transform(df_clean)
        # df_ncf = m_indexer.transform(df_ncf)
        
        # # L∆∞u d·ªØ li·ªáu ch√≠nh
        # df_ncf.select(
        #     F.col("userIndex").cast("integer"), 
        #     F.col("movieIndex").cast("integer"), 
        #     F.col("rating")
        # ).write.mode("overwrite").parquet(MODEL_PATHS["m3"])
        
        # # L∆∞u Mapping ƒë·ªÉ tra c·ª©u sau n√†y
        # spark.createDataFrame([(i, ) for i in u_indexer.labels], ["original_userId"]) \
        #     .write.mode("overwrite").parquet(MODEL_PATHS["m3_u_map"])
        # spark.createDataFrame([(i, ) for i in m_indexer.labels], ["original_movieId"]) \
        #     .write.mode("overwrite").parquet(MODEL_PATHS["m3_m_map"])

        print(f"\nüéâ HO√ÄN T·∫§T! D·ªØ li·ªáu s·∫°ch ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_PATH}")

    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}")
    finally:
        spark.stop()
        print("üîå Spark Session ƒë√£ ƒë√≥ng.")

if __name__ == "__main__":
    main()