import sys
import os
import time
import shutil
import gc
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql import functions as F

# --- C·∫§U H√åNH ---
INPUT_PATH = "data/processed/model1_rules"
OUTPUT_PATH = "checkpoints/model_1_rules/rules.parquet" 
TEMP_DIR = os.path.join(os.getcwd(), "spark_temp_data") 

# TƒÇNG minSupport l√™n m·ª©c an to√†n ƒë·ªÉ ch·∫°y th√¥ng lu·ªìng tr∆∞·ªõc
# N·∫øu v·∫´n l·ªói, h√£y tƒÉng l√™n 0.2 ho·∫∑c 0.5 ƒë·ªÉ test giao di·ªán tr∆∞·ªõc
MIN_SUPPORT = 0.15  
MIN_CONFIDENCE = 0.1

def main():
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Spark cho Training Model 1...")
    spark = SparkSession.builder \
        .appName("Train_Model_1_Rules") \
        .config("spark.driver.memory", "12g") \
        .config("spark.executor.memory", "12g") \
        .config("spark.driver.maxResultSize", "4g") \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.memory.offHeap.enabled", "true") \
        .config("spark.memory.offHeap.size", "4g") \
        .getOrCreate()

    # Gi·∫£m m·ª©c Log ƒë·ªÉ d·ªÖ theo d√µi l·ªói th·ª±c s·ª±
    spark.sparkContext.setLogLevel("ERROR")

    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {INPUT_PATH}...")
    if not os.path.exists(INPUT_PATH):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë·∫ßu v√†o.")
        return

    df = spark.read.parquet(INPUT_PATH)
    
    print("üßπ ƒêang lo·∫°i b·ªè phim tr√πng l·∫∑p...")
    df_clean = df.withColumn("items", F.array_distinct(F.col("items")))

    print("üíæ ƒêang n·∫°p d·ªØ li·ªáu v√†o RAM...")
    # S·ª≠ d·ª•ng MEMORY_AND_DISK ƒë·ªÉ n·∫øu thi·∫øu RAM s·∫Ω ghi t·∫°m ra ·ªï c·ª©ng thay v√¨ s·∫≠p Java
    df_clean.persist() 
    
    try:
        count = df_clean.count()
        print(f"‚úÖ ƒê√£ n·∫°p xong {count} d√≤ng d·ªØ li·ªáu.")

        # --- TRAIN ---
        print(f"üõ†  B·∫Øt ƒë·∫ßu Train FPGrowth (Support: {MIN_SUPPORT})...")
        start_time = time.time()
        
        fp = FPGrowth(itemsCol="items", 
                      minSupport=MIN_SUPPORT, 
                      minConfidence=MIN_CONFIDENCE)

        model = fp.fit(df_clean)
        print(f"‚è±  Train xong trong {round(time.time() - start_time, 2)} gi√¢y.")

        # --- K·∫æT QU·∫¢ ---
        # L∆∞u √Ω: AssociationRules l√† ph·∫ßn n·∫∑ng nh·∫•t g√¢y s·∫≠p Java
        rules = model.associationRules
        
        print("üìä ƒêang ki·ªÉm tra s·ªë l∆∞·ª£ng lu·∫≠t...")
        # S·ª≠ d·ª•ng persist cho rules tr∆∞·ªõc khi count
        rules.persist()
        rule_count = rules.count()
        print(f"üéâ ƒê√£ t√¨m th·∫•y {rule_count} lu·∫≠t k·∫øt h·ª£p!")

        if rule_count > 0:
            print("--- Top 5 lu·∫≠t m·∫°nh nh·∫•t ---")
            rules.sort(F.col("lift").desc()).show(5, truncate=False)
            
            print(f"üíæ ƒêang l∆∞u lu·∫≠t v√†o {OUTPUT_PATH}...")
            # Coalesce(1) gi√∫p l∆∞u th√†nh 1 file duy nh·∫•t n·∫øu d·ªØ li·ªáu kh√¥ng qu√° l·ªõn
            rules.coalesce(1).write.mode("overwrite").parquet(OUTPUT_PATH)
            print("‚úÖ L∆∞u th√†nh c√¥ng!")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y lu·∫≠t n√†o. H√£y gi·∫£m minSupport.")

    except Exception as e:
        print(f"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()