import sys
import os
import time
import shutil
import gc
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql import functions as F

# --- C·∫§U H√åNH ---
INPUT_PATH = "data/processed/model1_rules"
OUTPUT_PATH = "checkpoints/model_1_rulesv2/rules.parquet"
TEMP_DIR = os.path.join(os.getcwd(), "spark_temp_data") 

# --- C·∫§U H√åNH AN TO√ÄN TUY·ªÜT ƒê·ªêI (SURVIVAL MODE) ---
# 1. TƒÉng Support l√™n 3% (M·∫•t m·ªôt s·ªë phim ng√°ch, nh∆∞ng ƒë·∫£m b·∫£o ch·∫°y xong)
MIN_SUPPORT = 0.03      
# 2. Confidence 40% (Gi·ªØ lu·∫≠t ch·∫•t l∆∞·ª£ng)
MIN_CONFIDENCE = 0.4    
# 3. Lift 1.5 (Ch·∫∑n phim r√°c)
MIN_LIFT = 1.5           

# 4. Gi·∫£m s·ªë l∆∞·ª£ng phim t√≠nh to√°n xu·ªëng 30 (R·∫•t quan tr·ªçng ƒë·ªÉ gi·∫£m t·ªï h·ª£p)
MAX_ITEMS_PER_USER = 30   
# 5. Ch·ªâ d√πng 50% d·ªØ li·ªáu ƒë·ªÉ train (N·∫øu ch·∫°y th√†nh c√¥ng m·ªõi tƒÉng l√™n)
USER_SAMPLE_FRACTION = 0.5 

def main():
    # D·ªçn d·∫πp temp c≈©
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)

    print(f"üöÄ Kh·ªüi ƒë·ªông Spark (Survival Mode - 8GB)...")
    
    spark = SparkSession.builder \
        .appName("Train_Rules_Survival") \
        .config("spark.driver.memory", "6g") \
        .config("spark.executor.memory", "6g") \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.driver.maxResultSize", "2g") \
        .config("spark.memory.fraction", "0.6") \
        .config("spark.memory.storageFraction", "0.2") \
        .config("spark.local.dir", TEMP_DIR) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    spark.sparkContext.setCheckpointDir(os.path.join(TEMP_DIR, "checkpoints"))

    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
    if not os.path.exists(INPUT_PATH): return

    df = spark.read.parquet(INPUT_PATH)
    
    print(f"‚úÇÔ∏è ƒêang x·ª≠ l√Ω d·ªØ li·ªáu (Sample={USER_SAMPLE_FRACTION}, MaxItems={MAX_ITEMS_PER_USER})...")
    
    # 1. L·∫•y m·∫´u
    df = df.sample(withReplacement=False, fraction=USER_SAMPLE_FRACTION, seed=42)
    
    # 2. Slicing & Deduplicate
    df_clean = df.withColumn("items_distinct", F.array_distinct(F.col("items"))) \
                 .withColumn("items_sliced", F.slice(F.col("items_distinct"), 1, MAX_ITEMS_PER_USER)) \
                 .select(F.col("items_sliced").alias("items"))
    
    # 3. Repartition (Chia nh·ªè d·ªØ li·ªáu ra 200 g√≥i ƒë·ªÉ Executor kh√¥ng b·ªã ngh·∫πn)
    df_clean = df_clean.repartition(200)
    
    # 4. Checkpoint (C·∫Øt ƒë·ª©t RAM c≈©)
    df_clean = df_clean.checkpoint()
    
    count = df_clean.count()
    print(f"‚úÖ S·∫µn s√†ng train: {count} users.")

    print(f"üõ†  Train FPGrowth (Supp={MIN_SUPPORT})...")
    start_time = time.time()
    
    fp = FPGrowth(itemsCol="items", 
                  minSupport=MIN_SUPPORT, 
                  minConfidence=MIN_CONFIDENCE)
    
    try:
        model = fp.fit(df_clean)
        print(f"‚è±  Train xong trong {round(time.time() - start_time, 2)} gi√¢y.")

        print("üíæ ƒêang l·ªçc v√† l∆∞u k·∫øt qu·∫£...")
        rules = model.associationRules
        
        # L·ªçc lu·∫≠t ngay tr√™n lu·ªìng x·ª≠ l√Ω
        rules = rules.filter(F.size(F.col("antecedent")) <= 2)
        rules = rules.filter(F.col("lift") >= MIN_LIFT)
        
        # Chia nh·ªè file ƒë·∫ßu ra
        rules = rules.repartition(5)
        
        rules.write.mode("overwrite").parquet(OUTPUT_PATH)
        print(f"‚úÖ L∆ØU TH√ÄNH C√îNG! (Support={MIN_SUPPORT})")
        
        # Ki·ªÉm tra nhanh
        saved = spark.read.parquet(OUTPUT_PATH)
        print(f"üéâ S·ªë lu·∫≠t t√¨m ƒë∆∞·ª£c: {saved.count()}")
        
        # D·ªçn d·∫πp
        if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)

    except Exception as e:
        print(f"‚ùå L·ªñI: {e}")

    spark.stop()

if __name__ == "__main__":
    main()