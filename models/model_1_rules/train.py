import sys
import os
import time
import argparse
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql import functions as F


PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
from setting.config import INPUT_PATH_1, RESULT_PATH_1, MIN_SUPPORT, MIN_CONFIDENCE


def check_results_exist():
    """Ki·ªÉm tra xem th∆∞ m·ª•c k·∫øt qu·∫£ ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a"""
    if os.path.exists(RESULT_PATH_1):
        # Ki·ªÉm tra xem c√≥ file parquet b√™n trong kh√¥ng (th∆∞ m·ª•c kh√¥ng r·ªóng)
        if os.listdir(RESULT_PATH_1):
            return True
    return False

def main():
    # 0. X·ª≠ l√Ω tham s·ªë d√≤ng l·ªánh
    parser = argparse.ArgumentParser(description="Train Association Rules Model")
    parser.add_argument("--train", action="store_true", help="B·∫Øt bu·ªôc train l·∫°i d√π ƒë√£ c√≥ d·ªØ li·ªáu")
    args = parser.parse_args()

    # 1. Ki·ªÉm tra d·ªØ li·ªáu c≈©
    if check_results_exist() and not args.train:
        print(f"‚úÖ Lu·∫≠t k·∫øt h·ª£p ƒë√£ t·ªìn t·∫°i t·∫°i: {RESULT_PATH_1}")
        print("üöÄ B·ªè qua training. (S·ª≠ d·ª•ng --train n·∫øu b·∫°n mu·ªën c·∫≠p nh·∫≠t lu·∫≠t m·ªõi)")
        return

    print("üÜï B·∫Øt ƒë·∫ßu quy tr√¨nh hu·∫•n luy·ªán Model 1 (Association Rules)...")

    # 2. Kh·ªüi t·∫°o Spark
    spark = SparkSession.builder \
        .appName("Train_Model_1_Rules") \
        .config("spark.driver.memory", "12g") \
        .config("spark.executor.memory", "12g") \
        .config("spark.driver.maxResultSize", "4g") \
        .config("spark.sql.shuffle.partitions", "200") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")

    try:
        # 3. ƒê·ªçc d·ªØ li·ªáu
        print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {INPUT_PATH_1}...")
        if not os.path.exists(INPUT_PATH_1):
            print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë·∫ßu v√†o.")
            return

        df = spark.read.parquet(INPUT_PATH_1)
        
        # L√†m s·∫°ch d·ªØ li·ªáu: ƒê·∫£m b·∫£o c√°c m·∫£ng phim kh√¥ng b·ªã tr√πng l·∫∑p trong 1 giao d·ªãch
        df_clean = df.withColumn("items", F.array_distinct(F.col("items")))
        df_clean.persist() 

        # 4. Hu·∫•n luy·ªán FPGrowth
        print(f"üõ†  B·∫Øt ƒë·∫ßu Train FPGrowth (Support: {MIN_SUPPORT}, Confidence: {MIN_CONFIDENCE})...")
        start_time = time.time()
        
        fp = FPGrowth(itemsCol="items", 
                      minSupport=MIN_SUPPORT, 
                      minConfidence=MIN_CONFIDENCE)

        model = fp.fit(df_clean)
        print(f"‚è±  Train xong trong {round(time.time() - start_time, 2)} gi√¢y.")

        # 5. Tr√≠ch xu·∫•t Association Rules
        rules = model.associationRules
        rules.persist()
        
        rule_count = rules.count()
        print(f"üéâ ƒê√£ t√¨m th·∫•y {rule_count} lu·∫≠t k·∫øt h·ª£p!")

        if rule_count > 0:
            # 6. L∆∞u k·∫øt qu·∫£
            print(f"üíæ ƒêang l∆∞u lu·∫≠t v√†o {RESULT_PATH_1}...")
            # Coalesce(1) gom c·ª•m l·∫°i th√†nh 1 file ƒë·ªÉ API load cho nhanh
            rules.coalesce(1).write.mode("overwrite").parquet(RESULT_PATH_1)
            
            print("--- Top 5 lu·∫≠t c√≥ Lift cao nh·∫•t ---")
            rules.sort(F.col("lift").desc()).show(5, truncate=False)
            print("‚úÖ Ho√†n t·∫•t l∆∞u d·ªØ li·ªáu!")
        else:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y lu·∫≠t n√†o. G·ª£i √Ω: H√£y gi·∫£m MIN_SUPPORT.")

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()