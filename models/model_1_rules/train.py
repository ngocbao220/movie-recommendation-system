import sys
import os
import time
import shutil
import gc
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql import functions as F

# --- C·∫§U H√åNH ---
INPUT_PATH = "data/processed/model1_rules"
OUTPUT_PATH = "checkpoints/model_1_rules/rules.parquet" 
TEMP_DIR = os.path.join(os.getcwd(), "spark_temp_data") 

# --- C·∫§U H√åNH "GROWTH MODE" (C√ÇN B·∫∞NG GI·ªÆA S·ªê L∆Ø·ª¢NG V√Ä CH·∫§T L∆Ø·ª¢NG) ---

# 1. D√πng 100% d·ªØ li·ªáu
USER_SAMPLE_FRACTION = 1.0 

# 2. Support 1.5%: ƒê·ªß th·∫•p ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c phim Marvel, Harry Potter
MIN_SUPPORT = 0.015

# 3. Confidence 30%: ƒê·∫£m b·∫£o ƒë·ªô tin c·∫≠y kh√° (Xem A th√¨ 40% s·∫Ω xem B)
MIN_CONFIDENCE = 0.3   

# 4. Lift 1.5: L·ªçc b·ªè c√°c c·∫∑p phim "x√£ giao", ch·ªâ gi·ªØ l·∫°i quan h·ªá th√¢n thi·∫øt
MIN_LIFT = 1.5           

def main():
    # 0. D·ªçn d·∫πp th∆∞ m·ª•c t·∫°m ƒë·ªÉ tr√°nh l·ªói ·ªï c·ª©ng
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)

    print(f"üöÄ Kh·ªüi ƒë·ªông Spark (Growth Mode - Full Data)...")
    
    spark = SparkSession.builder \
        .appName("Train_Rules_Final") \
        .config("spark.driver.memory", "6g") \
        .config("spark.executor.memory", "6g") \
        .config("spark.sql.shuffle.partitions", "200") \
        .config("spark.driver.maxResultSize", "2g") \
        .config("spark.memory.fraction", "0.6") \
        .config("spark.memory.storageFraction", "0.2") \
        .config("spark.local.dir", TEMP_DIR) \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    spark.sparkContext.setCheckpointDir(os.path.join(TEMP_DIR, "checkpoints"))

    # 1. ƒê·ªåC D·ªÆ LI·ªÜU
    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {INPUT_PATH}...")
    if not os.path.exists(INPUT_PATH):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu ƒë·∫ßu v√†o.")
        return

    df = spark.read.parquet(INPUT_PATH)
    
    # 2. X·ª¨ L√ù D·ªÆ LI·ªÜU
    print(f"‚úÇÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu...")
    
    # L·∫•y m·∫´u (N·∫øu c·∫ßn test nhanh, gi·∫£m fraction xu·ªëng. Ch·∫°y th·∫≠t th√¨ ƒë·ªÉ 1.0)
    
    
    # CH√ö √ù: V√¨ process_data.py ƒë√£ l·ªçc Top 50 phim hay nh·∫•t/m·ªõi nh·∫•t r·ªìi
    # n√™n ta KH√îNG d√πng F.slice ·ªü ƒë√¢y n·ªØa. Ch·ªâ c·∫ßn array_distinct ƒë·ªÉ an to√†n.
    df_clean = df.withColumn("items", F.array_distinct(F.col("items")))
    
    # Repartition & Checkpoint ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ
    df_clean = df_clean.repartition(200).checkpoint()
    
    count = df_clean.count()
    print(f"‚úÖ S·∫µn s√†ng train tr√™n: {count} users.")

    # 3. TRAIN FPGROWTH
    print(f"üõ†  B·∫Øt ƒë·∫ßu Train FPGrowth (Supp={MIN_SUPPORT}, Conf={MIN_CONFIDENCE})...")
    start_time = time.time()
    
    fp = FPGrowth(itemsCol="items", 
                  minSupport=MIN_SUPPORT, 
                  minConfidence=MIN_CONFIDENCE)
    
    try:
        model = fp.fit(df_clean)
        print(f"‚è±  Train xong trong {round(time.time() - start_time, 2)} gi√¢y.")

        # 4. L·ªåC V√Ä L∆ØU K·∫æT QU·∫¢
        print("üíæ ƒêang sinh lu·∫≠t v√† l·ªçc...")
        rules = model.associationRules
        
        # --- B·ªò L·ªåC CH·∫§T L∆Ø·ª¢NG ---
        # 1. Antecedent <= 2: Gi·ªØ lu·∫≠t ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu
        rules = rules.filter(F.size(F.col("antecedent")) <= 2)
        
        # 2. Lift >= 2.0: Ch·ªâ l·∫•y m·ªëi quan h·ªá m·∫°nh
        rules = rules.filter(F.col("lift") >= MIN_LIFT)
        
        # (ƒê√£ b·ªè b·ªô l·ªçc support th·ª´a v√¨ minSupport ƒë√£ ch·∫∑n d∆∞·ªõi r·ªìi)

        # L∆∞u k·∫øt qu·∫£
        # Repartition(5) gi√∫p gom th√†nh 5 file l·ªõn, ƒë·ªçc nhanh h∆°n l√† 200 file nh·ªè
        rules = rules.repartition(5)
        rules.write.mode("overwrite").parquet(OUTPUT_PATH)
        
        print(f"‚úÖ L∆ØU TH√ÄNH C√îNG T·∫†I: {OUTPUT_PATH}")
        
        # 5. KI·ªÇM TRA NHANH
        saved = spark.read.parquet(OUTPUT_PATH)
        print(f"üéâ T·ªïng s·ªë lu·∫≠t t√¨m ƒë∆∞·ª£c: {saved.count()}")
        
        # D·ªçn d·∫πp r√°c
        if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)

    except Exception as e:
        print(f"‚ùå L·ªñI QU√Å TR√åNH TRAIN: {e}")

    spark.stop()

if __name__ == "__main__":
    main()