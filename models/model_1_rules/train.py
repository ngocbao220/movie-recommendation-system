import sys
import os
import time
from pyspark.sql import SparkSession
from pyspark.ml.fpm import FPGrowth
from pyspark.sql import functions as F

# --- C·∫§U H√åNH ---
INPUT_PATH = "data/processed/model1_rules"
OUTPUT_PATH = "checkpoints/model_1_rules/rules.parquet"

# --- THI·∫æT L·∫¨P H·ª¢P L√ù ---
# Khi ƒë√£ c·∫Øt ng·∫Øn d·ªØ li·ªáu, ta c√≥ th·ªÉ h·∫° Support xu·ªëng m·ª©c h·ª£p l√Ω h∆°n
MIN_SUPPORT = 0.03      # 3% (Thay v√¨ 0.1)
MIN_CONFIDENCE = 0.3    # 30%

# GI·ªöI H·∫†N D·ªÆ LI·ªÜU ƒê·ªÇ CH·ªêNG OOM
MAX_ITEMS_PER_USER = 50   # Ch·ªâ l·∫•y 50 phim/user
USER_SAMPLE_FRACTION = 0.5 # Ch·ªâ l·∫•y 50% s·ªë l∆∞·ª£ng user

def main():
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Spark (Optimized Mode)...")
    spark = SparkSession.builder \
        .appName("Train_Model_1_Rules") \
        .config("spark.driver.memory", "4g") \
        .config("spark.executor.memory", "4g") \
        .config("spark.sql.shuffle.partitions", "100") \
        .config("spark.driver.maxResultSize", "2g") \
        .config("spark.memory.offHeap.enabled", "true") \
        .config("spark.memory.offHeap.size", "1g") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")

    # 1. ƒê·ªçc d·ªØ li·ªáu
    print(f"üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ {INPUT_PATH}...")
    if not os.path.exists(INPUT_PATH):
        print("‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y data.")
        return

    df = spark.read.parquet(INPUT_PATH)
    total_users = df.count()
    print(f"üìä T·ªïng user ban ƒë·∫ßu: {total_users}")
    
    # 2. X·ª¨ L√ù D·ªÆ LI·ªÜU (CH√åA KH√ìA CH·ªêNG CRASH)
    print("‚úÇÔ∏è ƒêang t·ªëi ∆∞u h√≥a d·ªØ li·ªáu...")
    
    # B∆∞·ªõc A: L·∫•y m·∫´u ng·∫´u nhi√™n (Sampling)
    df_sampled = df.sample(withReplacement=False, fraction=USER_SAMPLE_FRACTION, seed=42)
    
    # B∆∞·ªõc B: C·∫Øt ng·∫Øn Transaction (Slicing) & X√≥a tr√πng l·∫∑p
    # slice(col, start, length): L·∫•y t·ª´ ph·∫ßn t·ª≠ s·ªë 1, l·∫•y t·ªëi ƒëa 50 ph·∫ßn t·ª≠
    df_clean = df_sampled.withColumn("items_distinct", F.array_distinct(F.col("items"))) \
                         .withColumn("items_sliced", F.slice(F.col("items_distinct"), 1, MAX_ITEMS_PER_USER)) \
                         .select(F.col("items_sliced").alias("items"))
    
    # Cache l·∫°i d·ªØ li·ªáu s·∫°ch
    df_clean.cache()
    clean_count = df_clean.count()
    print(f"‚úÖ D·ªØ li·ªáu sau khi x·ª≠ l√Ω: {clean_count} users (Max {MAX_ITEMS_PER_USER} items/user).")

    # 3. TRAIN
    print(f"üõ†  B·∫Øt ƒë·∫ßu Train FPGrowth (Support: {MIN_SUPPORT}, Conf: {MIN_CONFIDENCE})...")
    start_time = time.time()
    
    fp = FPGrowth(itemsCol="items", 
                  minSupport=MIN_SUPPORT, 
                  minConfidence=MIN_CONFIDENCE)
    
    model = fp.fit(df_clean)
    print(f"‚è±  Train xong trong {round(time.time() - start_time, 2)} gi√¢y.")

    # 4. L∆ØU K·∫æT QU·∫¢
    print("üíæ ƒêang sinh lu·∫≠t v√† l∆∞u...")
    try:
        rules = model.associationRules
        
        # --- T·ªêI ∆ØU B·ªò NH·ªö KHI L∆ØU ---
        # Ch·ªâ gi·ªØ l·∫°i c√°c lu·∫≠t ng·∫Øn (Antecedent <= 2)
        # Lu·∫≠t d√†i (VD: Xem A,B,C,D -> E) r·∫•t t·ªën b·ªô nh·ªõ v√† √≠t c√≥ gi√° tr·ªã g·ª£i √Ω th·ª±c t·∫ø
        print("   -> L·ªçc b·ªè c√°c lu·∫≠t qu√° d√†i (Antecedent > 2)...")
        rules_filtered = rules.filter(F.size(F.col("antecedent")) <= 2)
        
        # L∆∞u ra Parquet
        rules_filtered.write.mode("overwrite").parquet(OUTPUT_PATH)
        print("‚úÖ L∆∞u th√†nh c√¥ng!")
        
        # 5. KI·ªÇM TRA
        print("üìä Ki·ªÉm tra k·∫øt qu·∫£...")
        saved_rules = spark.read.parquet(OUTPUT_PATH)
        count_rules = saved_rules.count()
        print(f"üéâ T·ªïng s·ªë lu·∫≠t t√¨m th·∫•y: {count_rules}")
        
        if count_rules > 0:
            saved_rules.sort(F.col("lift").desc()).show(5, truncate=False)
            
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u: {e}")
        import traceback
        traceback.print_exc()

    spark.stop()

if __name__ == "__main__":
    main()